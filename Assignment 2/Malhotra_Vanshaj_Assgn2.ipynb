{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ec5359",
   "metadata": {},
   "source": [
    "# Assignment 2: Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3433711",
   "metadata": {},
   "source": [
    "## Programming Question 1: Implementing Deep FNN\n",
    "Through this assignment you learn how to use `torch.nn` module of torch to implement a neural network in only few lines of code.\n",
    "\n",
    "We first import all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6521ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import torch and the nn module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# We import NumPy and tqdm\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# We need few items from Scikit-Learn\n",
    "import sklearn.datasets as DataSets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Also we need to plot few curves\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e517ef1",
   "metadata": {},
   "source": [
    "### Data Splitting: Make Training Mini-Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b32adbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 10000 float64 int64 2.712949015269261 1\n",
      "float64 float64\n"
     ]
    }
   ],
   "source": [
    "dataset = DataSets.make_classification(n_samples = 10000\n",
    "                                       , n_features = 20\n",
    "                                       , n_informative=5\n",
    "                                       , n_redundant=15\n",
    "                                       , random_state=1)\n",
    "X, v = dataset\n",
    "print(X.size, v.size, X.dtype, v.dtype, X[0,0], v[0])\n",
    "v = v.astype(np.float64)\n",
    "print(X.dtype, v.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2a6c00",
   "metadata": {},
   "source": [
    "Below you can implement the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6168c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitter(X, v, batch_size, train_size):\n",
    "    '''\n",
    "    X is list of data-points\n",
    "    v is list of labels\n",
    "    train_size is the fraction of data-points used for training\n",
    "    '''\n",
    "    X_train, X_test, v_train, v_test = train_test_split(X, v\n",
    "        , train_size= 0.8\n",
    "        , shuffle = True\n",
    "        )\n",
    "    X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "    v_train = torch.tensor(v_train,dtype=torch.float32).reshape(-1,1)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    v_test = torch.tensor(v_test, dtype=torch.float32).reshape(-1,1)\n",
    "    batch_indx = torch.arange(X_train.shape[0])\n",
    "    return X_train, v_train, X_test, v_test, batch_indx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c87e1d",
   "metadata": {},
   "source": [
    "### Basic Classes\n",
    "\n",
    "For this task you can use `nn.Layer()`. You mainly need to complete the following `class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ceba5e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(20,64)\n",
    "        self.active1 = nn.ReLU()\n",
    "\n",
    "        self.layer2 = nn.Linear(64,32)\n",
    "        self.active2 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Linear(32,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.active1(self.layer1(x))\n",
    "        x = self.active2(self.layer2(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc167c",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "We now learn how we can use the autograd of PyTorch to implement backpropagation. Simply complete the following code and run it to see how autograd works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "453c2fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, instantiate the model \n",
    "myModel = myClassifier()\n",
    "\n",
    "# now, define the loss\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# split the dataset via function data_splitter()\n",
    "X_train, v_train, X_test, v_test, batch_indx = data_splitter(X, v, 100, 0.8)\n",
    "\n",
    "# take a sample point from training dataset\n",
    "x = X_train[0].unsqueeze(0)   \n",
    "v = v_train[0].unsqueeze(0)  \n",
    "\n",
    "# forward pass\n",
    "y = myModel(x)\n",
    "Loss = loss_fn(y, v)\n",
    "\n",
    "# backward pass\n",
    "Loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd464b52",
   "metadata": {},
   "source": [
    "We can now access the computed gradient of the loss with respect to the output bias using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8470a2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4847])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myModel.output.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d243b963",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "We can further define the optimizer for our model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f6f39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, instantiate the model \n",
    "myModel = myClassifier()\n",
    "\n",
    "# now define the optimizer\n",
    "optimizer = torch.optim.Adam(myModel.parameters()\n",
    "                        , lr=0.0001 # this specifies the learning rate\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bce652",
   "metadata": {},
   "source": [
    "### Implementing Training and Test Loop\n",
    "Now that we have all the components, we can simply implement the training loop. Complete the following code to get the training done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f89538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model):\n",
    "    # define the loss and optimizer\n",
    "    loss_fn = nn.BCELoss() # binary cross-entropy\n",
    "    optimizer = # complete <use Adam>\n",
    "\n",
    "    # set the training parameters\n",
    "    n_epochs = 300   # number of epochs\n",
    "    batch_size = 40  # batch size\n",
    "\n",
    "    # specify training and test datasets and the batch indices\n",
    "    # use data_splitter() and X, v are generated by Scikit-Learn\n",
    "    X_train, v_train, X_test, v_test, batch_indx = # complete \n",
    "\n",
    "    # make empty list to save training and test risk\n",
    "    train_risk = []\n",
    "    test_risk = []\n",
    "\n",
    "    # training loop\n",
    "\n",
    "    # we visualize the training progress via tqdm\n",
    "    with tqdm(range(n_epochs), unit=\"epoch\") as epoch_bar:\n",
    "        epoch_bar.set_description(\"training loop\")\n",
    "        for epoch in epoch_bar:\n",
    "\n",
    "            # tell pytorch that you start training\n",
    "            model.train()\n",
    "\n",
    "            for indx in batch_indx:\n",
    "                # take a batch of samples\n",
    "                X_batch = # complete\n",
    "                v_batch = # complete\n",
    "\n",
    "                # pass forward the mini-batch\n",
    "                y_batch = # complete\n",
    "\n",
    "                # compute the loss\n",
    "                loss = # complete\n",
    "\n",
    "                # backward pass\n",
    "                # first make gradient zero\n",
    "                optimizer.zero_grad()\n",
    "                # then, compute the gradient of loss\n",
    "                # complete\n",
    "\n",
    "                # now update weights by one optimization step\n",
    "                optimizer.step()\n",
    "\n",
    "            # we are done with one epoch\n",
    "            # we now evaluate training and test risks\n",
    "            # first we tell pytorch we are doing evaluation\n",
    "            model.eval()\n",
    "\n",
    "            # now we evaluate the training risk\n",
    "            y_train = # complete\n",
    "            CE_train = # complete\n",
    "            train_risk.append(CE_train.item())\n",
    "\n",
    "            # then we evaluate the test risk\n",
    "            y_test = # complete\n",
    "            CE_test = # complete\n",
    "            test_risk.append(CE_test.item())\n",
    "        return train_risk, test_risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cf7430",
   "metadata": {},
   "source": [
    "We can now try training our model by passing the model to the training loop. Complete the following code to train the model and plot the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfc551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = myClassifier()\n",
    "train_risk, test_risk = # complete\n",
    "\n",
    "# complete <plot training risk>\n",
    "# complete <plot test risk>\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3471ba",
   "metadata": {},
   "source": [
    "## Programming Question 2: MNIST Dataset\n",
    "We play around a bit with the datasets in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbb1155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as DS\n",
    "import torchvision.transforms as transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939178a2",
   "metadata": {},
   "source": [
    "Import the MNIST dataset by running the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b12c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = DS.MNIST('./data',train=True,transform=transform.ToTensor(),download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8cc082",
   "metadata": {},
   "source": [
    "We now build a function `myBatcher()` which gets the batch size as the input and returns a list containing multiple batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e835629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myBatcher(batch_size):\n",
    "    batch_list = []\n",
    "    num_batches = # COMPLETE\n",
    "    \n",
    "    for j in range(num_batches):\n",
    "        batch_x = torch.zeros(batch_size,784)\n",
    "        batch_v = torch.zeros(batch_size)\n",
    "        for i in range(batch_size):\n",
    "            batch_x[i] = # COMPLETE\n",
    "            batch_v[i] = # COMPLETE\n",
    "            \n",
    "        batch = (batch_x,batch_v)\n",
    "        batch_list.append(batch)\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94dc68b",
   "metadata": {},
   "source": [
    "Let's try the implemented function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_list = myBatcher(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1508",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
